{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16389d55",
   "metadata": {},
   "source": [
    "First it'd be best to setup the environment. I've copied the raw courses from: \n",
    "https://gist.github.com/pat-coady/26fafa10b4d14234bfde0bb58277786d\n",
    "\n",
    "Because this would take a while and be pretty tedious to do myself.\n",
    "\n",
    "But we'd still need to convert this to some other format. \n",
    "\n",
    "I think the best thing to do first would be to setup the environment, then from there work on the reinforcement learning part of it.\n",
    "\n",
    "Now that the environment is setup, we need to setup the target and behaviour policies.\n",
    "\n",
    "The target will just be a greedy deterministic policy.\n",
    "\n",
    "The behaviour policy will be epsilon greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09a7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Racetracks\n",
    "\n",
    "# Left Race Track from Figure 5.5\n",
    "big_course = ['WWWWWWWWWWWWWWWWWW',\n",
    "              'WWWWooooooooooooo+',\n",
    "              'WWWoooooooooooooo+',\n",
    "              'WWWoooooooooooooo+',\n",
    "              'WWooooooooooooooo+',\n",
    "              'Woooooooooooooooo+',\n",
    "              'Woooooooooooooooo+',\n",
    "              'WooooooooooWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWWooooooWWWWWWWW',\n",
    "              'WWWWooooooWWWWWWWW',\n",
    "              'WWWW------WWWWWWWW']\n",
    "\n",
    "# Tiny course for debug\n",
    "\n",
    "tiny_course = ['WWWWWW',\n",
    "               'Woooo+',\n",
    "               'Woooo+',\n",
    "               'WooWWW',\n",
    "               'WooWWW',\n",
    "               'WooWWW',\n",
    "               'WooWWW',\n",
    "               'W--WWW',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a06fd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class RaceTrack:\n",
    "    def __init__(self, course: list[str]):\n",
    "        self.course = self._load_course(course)\n",
    "        self.car_location = self._start()\n",
    "        self.velocity = (0, 0)\n",
    "        self.actions = [(x, y) for x in range(-1, 2) for y in range(-1, 2)]\n",
    "        self.action_idxs = np.arange(9)\n",
    "\n",
    "    def _pick_action(self, state: tuple[int, int, int, int], policy: np.array) -> int:\n",
    "        p_distribution = policy[*state]\n",
    "        # Action array should be of size 9\n",
    "        # Need to sample from it\n",
    "        action_idx = np.random.choice(self.action_idxs, 1, p=p_distribution)[0]\n",
    "        return action_idx\n",
    "\n",
    "    def _load_course(self, string_course: list[str]) -> np.array:\n",
    "        course = np.zeros((len(string_course), len(string_course[0])), dtype=np.int16)\n",
    "        course_dict = {\"W\": -1, \"o\": 0, \"-\": 1, \"+\": 2}\n",
    "\n",
    "        for i in range(len(course)):\n",
    "            for j in range(len(course[i])):\n",
    "                course[i, j] = course_dict[string_course[i][j]]\n",
    "\n",
    "        return course\n",
    "    \n",
    "    def _start(self) -> tuple[int, int]:\n",
    "        rows, cols = np.where(self.course == 1)\n",
    "        coords = list(zip(rows, cols))\n",
    "        start = random.randint(0, len(coords)-1)\n",
    "\n",
    "        return coords[start]\n",
    "    \n",
    "    def _restart(self): # If we hit the car boundary\n",
    "        self.car_location = self._start()\n",
    "        self.velocity = (0, 0)\n",
    "    \n",
    "    def _apply_velocity(self, action: tuple[int, int]):\n",
    "        y_vel = min(max(self.velocity[0] + action[0], 0), 5) # Bound to between 0-5\n",
    "        x_vel = min(max(self.velocity[1] + action[1], 0), 5)\n",
    "        self.velocity = (y_vel, x_vel)\n",
    "\n",
    "    def _apply_movement(self) -> str:\n",
    "        result = self._check_bounds()\n",
    "\n",
    "        if result == \"invalid\":\n",
    "            self._restart()\n",
    "            return \"restarted\"\n",
    "        elif result == \"complete\":\n",
    "            return \"done\"\n",
    "        else:\n",
    "            return \"continuing\"\n",
    "    \n",
    "    def _check_bounds(self) -> str:\n",
    "        y_vel, x_vel = self.velocity\n",
    "        y, x = self.car_location\n",
    "\n",
    "        max_x = len(self.course[0])\n",
    "        all_positions = [(y, x)]\n",
    "        max_vel = max(x_vel, y_vel)\n",
    "        y_vel_count, x_vel_count = y_vel, x_vel\n",
    "        y_temp, x_temp = y, x\n",
    "\n",
    "        for _ in range(max_vel):\n",
    "            if y_vel_count > 0 and x_vel_count > 0:\n",
    "                y_temp -= 1\n",
    "                x_temp += 1\n",
    "                if y_temp < 0 or x_temp >= max_x:\n",
    "                    return \"invalid\"\n",
    "                all_positions.append((y_temp, x_temp))\n",
    "                y_vel_count -= 1\n",
    "                x_vel_count -= 1\n",
    "            elif y_vel_count > 0:\n",
    "                y_temp -= 1\n",
    "                if y_temp < 0:\n",
    "                    return \"invalid\"\n",
    "                all_positions.append((y_temp, x_temp))\n",
    "                y_vel_count -= 1\n",
    "            else:\n",
    "                x_temp += 1\n",
    "                if x_temp >= max_x:\n",
    "                    return \"invalid\"\n",
    "                all_positions.append((y_temp, x_temp))\n",
    "                x_vel_count -= 1\n",
    "\n",
    "        all_position_values = []\n",
    "\n",
    "        for i, j in all_positions:\n",
    "            all_position_values.append(self.get_course()[i, j])\n",
    "\n",
    "        if -1 in all_position_values:\n",
    "            return \"invalid\"\n",
    "        elif 2 in all_position_values:\n",
    "            return \"complete\"\n",
    "        else:\n",
    "            self.car_location = all_positions[-1]\n",
    "            return \"valid\"\n",
    "\n",
    "    def get_course(self) -> np.array:\n",
    "        return self.course\n",
    "\n",
    "    def generate_random_episode(self): # For testing\n",
    "        self.car_location = self._start()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action_idx = random.randint(0, len(self.actions)-1)\n",
    "            action = self.actions[action_idx]\n",
    "            self._apply_velocity(action)\n",
    "            result = self._apply_movement()\n",
    "            self.state_action_trajectory.append((self.car_location, action_idx))\n",
    "            self.running_reward -= 1\n",
    "\n",
    "            if result == \"done\":\n",
    "                done = True\n",
    "\n",
    "    def generate_episode(self, policy):\n",
    "        self.car_location = self._start()\n",
    "        done = False\n",
    "        state_action_trajectory = []\n",
    "\n",
    "        while not done:\n",
    "            velocity_good = False\n",
    "            y, x = self.car_location\n",
    "            y_vel, x_vel = self.velocity\n",
    "            old_velocity = self.velocity\n",
    "\n",
    "            while not velocity_good: # To check if velocity is not (0,0) as per problem description\n",
    "                action_idx = self._pick_action((y, x, y_vel, x_vel), policy) # Take action\n",
    "                action = self.actions[action_idx]\n",
    "                self._apply_velocity(action)\n",
    "\n",
    "                if self.velocity != (0, 0): # If it's not (0, 0), then pass\n",
    "                    velocity_good = True\n",
    "                else: # Else reset velocity and pick something different\n",
    "                    self.velocity = old_velocity\n",
    "\n",
    "            result = self._apply_movement()\n",
    "            state_action_trajectory.append((*self.car_location, *self.velocity, action_idx))\n",
    "\n",
    "            if result == \"done\":\n",
    "                done = True\n",
    "\n",
    "        return state_action_trajectory\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9905c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Iteration:  10\n",
      "Iteration:  20\n",
      "Iteration:  30\n",
      "Iteration:  40\n",
      "Iteration:  50\n",
      "Iteration:  60\n",
      "Iteration:  70\n",
      "Iteration:  80\n",
      "Iteration:  90\n",
      "Iteration:  100\n",
      "Iteration:  110\n",
      "Iteration:  120\n",
      "Iteration:  130\n",
      "Iteration:  140\n",
      "Iteration:  150\n",
      "Iteration:  160\n",
      "Iteration:  170\n",
      "Iteration:  180\n",
      "Iteration:  190\n",
      "Iteration:  200\n",
      "Iteration:  210\n",
      "Iteration:  220\n",
      "Iteration:  230\n",
      "Iteration:  240\n",
      "Iteration:  250\n",
      "Iteration:  260\n",
      "Iteration:  270\n",
      "Iteration:  280\n",
      "Iteration:  290\n",
      "Iteration:  300\n",
      "Iteration:  310\n",
      "Iteration:  320\n",
      "Iteration:  330\n",
      "Iteration:  340\n",
      "Iteration:  350\n",
      "Iteration:  360\n",
      "Iteration:  370\n",
      "Iteration:  380\n",
      "Iteration:  390\n",
      "Iteration:  400\n",
      "Iteration:  410\n",
      "Iteration:  420\n",
      "Iteration:  430\n",
      "Iteration:  440\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "course = big_course\n",
    "\n",
    "actions = np.array([(x, y) for x in range(-1, 2) for y in range(-1, 2)])\n",
    "\n",
    "# Setup target and behaviour policies\n",
    "# Actions here could be represented as an array of size (9) or (3, 3)\n",
    "# If it were 9, then the actions are grouped to be (-1, -1), (-1, 0), ..., (1, 1) etc.\n",
    "# If it were (3, 3), then there's 3 possible actions for each axis, but I think this would be more annoying to implement\n",
    "\n",
    "target_q_values = np.zeros((len(course), len(course[0]), 6, 6, 9)) # len(x), len(y), 6 speeds up and down (0-6), 9 possible actions. If it were state values, we'd just need v = x, y, 5, 5\n",
    "target_policy = np.zeros((len(course), len(course[0]), 6, 6, 9)) + 4 # Need an array for each state, default value set to (0, 0)\n",
    "c_values = np.zeros(target_q_values.shape) # Need the same shape for our running importance sampling sum\n",
    "\n",
    "behaviour_policy = np.zeros((len(course), len(course[0]), 6, 6, 9)) + (1/9) # Begin as equiprobable policy, will be stochastic\n",
    "behaviour_q_values = np.zeros(behaviour_policy.shape)\n",
    "behaviour_policy_counts = np.zeros(behaviour_policy.shape)\n",
    "\n",
    "gamma = 1\n",
    "epsilon = 0.05\n",
    "default_probability = epsilon/len(actions)\n",
    "\n",
    "race_track = RaceTrack(course)\n",
    "\n",
    "def on_policy_monte_carlo_control(race_track: RaceTrack, policy: np.array, q_values: np.array, counts: np.array, gamma: float, epsilon: float, iterations: int = 100000):\n",
    "    default_probability = epsilon/9\n",
    "    for i in range(iterations):\n",
    "        state_action_trajectory = race_track.generate_episode(policy)\n",
    "        current_return = 0\n",
    "\n",
    "        for timestep, state_action in enumerate(state_action_trajectory[::-1]):\n",
    "            current_return = gamma * current_return + -1 # Taking advantage of how the reward structure for the problem is\n",
    "            # We don't need the explicit overall reward as it's just the negative of the length of the trajectory\n",
    "\n",
    "            if state_action not in state_action_trajectory[:timestep]:\n",
    "                counts[*state_action] += 1\n",
    "                q_values[*state_action] += (current_return-q_values[*state_action])/counts[*state_action]\n",
    "                best_action = np.argmax(q_values[*state_action[:-1]])\n",
    "                q_values[*state_action[:-1]] = default_probability\n",
    "                q_values[*state_action[:-1], best_action] += (1-epsilon)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "\n",
    "                \n",
    "\n",
    "on_policy_monte_carlo_control(race_track, behaviour_policy, behaviour_q_values, behaviour_policy_counts, gamma, epsilon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "cb506291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0.11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.005555555555555556"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((8, 3, 2) == (np.int64(8), np.int64(3), np.int64(1)))\n",
    "len(actions)\n",
    "print(default_probability)\n",
    "0.05/9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
