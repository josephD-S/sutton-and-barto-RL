{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16389d55",
   "metadata": {},
   "source": [
    "First it'd be best to setup the environment. I've copied the raw courses from: \n",
    "https://gist.github.com/pat-coady/26fafa10b4d14234bfde0bb58277786d\n",
    "\n",
    "Because this would take a while and be pretty tedious to do myself.\n",
    "\n",
    "But we'd still need to convert this to some other format. \n",
    "\n",
    "I think the best thing to do first would be to setup the environment, then from there work on the reinforcement learning part of it.\n",
    "\n",
    "Now that the environment is setup, we need to setup the target and behaviour policies.\n",
    "\n",
    "The target will just be a greedy deterministic policy.\n",
    "\n",
    "The behaviour policy will be epsilon greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09a7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Racetracks\n",
    "\n",
    "# Left Race Track from Figure 5.5\n",
    "big_course = ['WWWWWWWWWWWWWWWWWW',\n",
    "              'WWWWooooooooooooo+',\n",
    "              'WWWoooooooooooooo+',\n",
    "              'WWWoooooooooooooo+',\n",
    "              'WWooooooooooooooo+',\n",
    "              'Woooooooooooooooo+',\n",
    "              'Woooooooooooooooo+',\n",
    "              'WooooooooooWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWWooooooWWWWWWWW',\n",
    "              'WWWWooooooWWWWWWWW',\n",
    "              'WWWW------WWWWWWWW']\n",
    "\n",
    "# Tiny course for debug\n",
    "\n",
    "tiny_course = ['WWWWWW',\n",
    "               'Woooo+',\n",
    "               'Woooo+',\n",
    "               'WooWWW',\n",
    "               'WooWWW',\n",
    "               'WooWWW',\n",
    "               'WooWWW',\n",
    "               'W--WWW',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "6bcac298",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RaceTrack:\n",
    "    def __init__(self, course: list[str]):\n",
    "        self.course = self._load_course(course)\n",
    "        self.car_location = self._start()\n",
    "        self.velocity = (0, 0)\n",
    "        self.actions = [(x, y) for x in range(-1, 2) for y in range(-1, 2)]\n",
    "        self.action_idxs = np.arange(9)\n",
    "\n",
    "    def _pick_action(self, state: tuple[int, int, int, int], policy: np.array) -> int:\n",
    "        p_distribution = policy[*state]\n",
    "        # Action array should be of size 9\n",
    "        # Need to sample from it\n",
    "        action_idx = np.random.choice(self.action_idxs, 1, p=p_distribution)[0]\n",
    "        return action_idx\n",
    "\n",
    "    def _load_course(self, string_course: list[str]) -> np.array:\n",
    "        course = np.zeros((len(string_course), len(string_course[0])), dtype=np.int16)\n",
    "        course_dict = {\"W\": -1, \"o\": 0, \"-\": 1, \"+\": 2}\n",
    "\n",
    "        for i in range(len(course)):\n",
    "            for j in range(len(course[i])):\n",
    "                course[i, j] = course_dict[string_course[i][j]]\n",
    "\n",
    "        return course\n",
    "    \n",
    "    def _start(self) -> tuple[int, int]:\n",
    "        rows, cols = np.where(self.course == 1)\n",
    "        coords = list(zip(rows, cols))\n",
    "        start = random.randint(0, len(coords)-1)\n",
    "\n",
    "        return coords[start]\n",
    "    \n",
    "    def _restart(self): # If we hit the car boundary\n",
    "        self.car_location = self._start()\n",
    "        self.velocity = (0, 0)\n",
    "    \n",
    "    def _apply_velocity(self, action: tuple[int, int]):\n",
    "        y_vel = min(max(self.velocity[0] + action[0], 0), 5) # Bound to between 0-5\n",
    "        x_vel = min(max(self.velocity[1] + action[1], 0), 5)\n",
    "        self.velocity = (y_vel, x_vel)\n",
    "\n",
    "    def _apply_movement(self) -> str:\n",
    "        result = self._check_bounds()\n",
    "\n",
    "        if result == \"invalid\":\n",
    "            self._restart()\n",
    "            return \"restarted\"\n",
    "        elif result == \"complete\":\n",
    "            return \"done\"\n",
    "        else:\n",
    "            return \"continuing\"\n",
    "    \n",
    "    def _check_bounds(self) -> str:\n",
    "        y_vel, x_vel = self.velocity\n",
    "        y, x = self.car_location\n",
    "\n",
    "        max_x = len(self.course[0])\n",
    "        all_positions = [(y, x)]\n",
    "        max_vel = max(x_vel, y_vel)\n",
    "        y_vel_count, x_vel_count = y_vel, x_vel\n",
    "        y_temp, x_temp = y, x\n",
    "\n",
    "        for _ in range(max_vel):\n",
    "            if y_vel_count > 0 and x_vel_count > 0:\n",
    "                y_temp -= 1\n",
    "                x_temp += 1\n",
    "                if y_temp < 0 or x_temp >= max_x:\n",
    "                    return \"invalid\"\n",
    "                all_positions.append((y_temp, x_temp))\n",
    "                y_vel_count -= 1\n",
    "                x_vel_count -= 1\n",
    "            elif y_vel_count > 0:\n",
    "                y_temp -= 1\n",
    "                if y_temp < 0:\n",
    "                    return \"invalid\"\n",
    "                all_positions.append((y_temp, x_temp))\n",
    "                y_vel_count -= 1\n",
    "            else:\n",
    "                x_temp += 1\n",
    "                if x_temp >= max_x:\n",
    "                    return \"invalid\"\n",
    "                all_positions.append((y_temp, x_temp))\n",
    "                x_vel_count -= 1\n",
    "\n",
    "        all_position_values = []\n",
    "\n",
    "        for i, j in all_positions:\n",
    "            all_position_values.append(self.get_course()[i, j])\n",
    "\n",
    "        if -1 in all_position_values:\n",
    "            return \"invalid\"\n",
    "        elif 2 in all_position_values:\n",
    "            return \"complete\"\n",
    "        else:\n",
    "            self.car_location = all_positions[-1]\n",
    "            return \"valid\"\n",
    "\n",
    "    def get_course(self) -> np.array:\n",
    "        return self.course\n",
    "\n",
    "    def generate_random_episode(self): # For testing\n",
    "        self.car_location = self._start()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action_idx = random.randint(0, len(self.actions)-1)\n",
    "            action = self.actions[action_idx]\n",
    "            self._apply_velocity(action)\n",
    "            result = self._apply_movement()\n",
    "            self.state_action_trajectory.append((self.car_location, action_idx))\n",
    "            self.running_reward -= 1\n",
    "\n",
    "            if result == \"done\":\n",
    "                done = True\n",
    "\n",
    "    def generate_episode(self, policy):\n",
    "        self._restart()\n",
    "        done = False\n",
    "        state_action_trajectory = []\n",
    "\n",
    "        while not done:\n",
    "            velocity_good = False\n",
    "            y, x = self.car_location\n",
    "            y_vel, x_vel = self.velocity\n",
    "            old_velocity = self.velocity\n",
    "\n",
    "            while not velocity_good: # To check if velocity is not (0,0) as per problem description\n",
    "                action_idx = self._pick_action((y, x, y_vel, x_vel), policy) # Take action\n",
    "                action = self.actions[action_idx]\n",
    "                self._apply_velocity(action)\n",
    "\n",
    "                if self.velocity != (0, 0): # If it's not (0, 0), then pass\n",
    "                    velocity_good = True\n",
    "                else: # Else reset velocity and pick something different\n",
    "                    self.velocity = old_velocity\n",
    "\n",
    "            result = self._apply_movement()\n",
    "            state_action_trajectory.append((*self.car_location, *self.velocity, action_idx))\n",
    "\n",
    "            if result == \"done\":\n",
    "                done = True\n",
    "\n",
    "        return state_action_trajectory\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "b9905c3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RaceTrack' object has no attribute '_reset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[294]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     55\u001b[39m         plt.grid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     56\u001b[39m         plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[43mon_policy_monte_carlo_control\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrace_track\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehaviour_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehaviour_q_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehaviour_policy_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[294]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mon_policy_monte_carlo_control\u001b[39m\u001b[34m(race_track, policy, q_values, counts, gamma, epsilon, iterations)\u001b[39m\n\u001b[32m     31\u001b[39m episode_length_history = []\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     state_action_trajectory = \u001b[43mrace_track\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     current_return = \u001b[32m0\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m timestep, state_action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(state_action_trajectory[::-\u001b[32m1\u001b[39m]):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[293]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mRaceTrack.generate_episode\u001b[39m\u001b[34m(self, policy)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_episode\u001b[39m(\u001b[38;5;28mself\u001b[39m, policy):\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m()\n\u001b[32m    118\u001b[39m     done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    119\u001b[39m     state_action_trajectory = []\n",
      "\u001b[31mAttributeError\u001b[39m: 'RaceTrack' object has no attribute '_reset'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "course = big_course\n",
    "\n",
    "actions = np.array([(x, y) for x in range(-1, 2) for y in range(-1, 2)])\n",
    "\n",
    "# Setup target and behaviour policies\n",
    "# Actions here could be represented as an array of size (9) or (3, 3)\n",
    "# If it were 9, then the actions are grouped to be (-1, -1), (-1, 0), ..., (1, 1) etc.\n",
    "# If it were (3, 3), then there's 3 possible actions for each axis, but I think this would be more annoying to implement\n",
    "\n",
    "target_q_values = np.zeros((len(course), len(course[0]), 6, 6, 9)) # len(x), len(y), 6 speeds up and down (0-6), 9 possible actions. If it were state values, we'd just need v = x, y, 5, 5\n",
    "target_policy = np.zeros((len(course), len(course[0]), 6, 6, 9)) + 4 # Need an array for each state, default value set to (0, 0)\n",
    "c_values = np.zeros(target_q_values.shape) # Need the same shape for our running importance sampling sum\n",
    "\n",
    "behaviour_policy = np.zeros((len(course), len(course[0]), 6, 6, 9)) + (1/9) # Begin as equiprobable policy, will be stochastic\n",
    "behaviour_q_values = np.zeros(behaviour_policy.shape)\n",
    "behaviour_policy_counts = np.zeros(behaviour_policy.shape)\n",
    "\n",
    "gamma = 1\n",
    "epsilon = 0.05\n",
    "default_probability = epsilon/len(actions)\n",
    "\n",
    "race_track = RaceTrack(course)\n",
    "\n",
    "plt.ion()\n",
    "def on_policy_monte_carlo_control(race_track: RaceTrack, policy: np.array, q_values: np.array, counts: np.array, gamma: float, epsilon: float, iterations: int = 100000):\n",
    "    default_probability = epsilon/9\n",
    "    episode_length_history = []\n",
    "    for i in range(iterations):\n",
    "        state_action_trajectory = race_track.generate_episode(policy)\n",
    "        current_return = 0\n",
    "\n",
    "        for timestep, state_action in enumerate(state_action_trajectory[::-1]):\n",
    "            current_return = gamma * current_return + -1 # Taking advantage of how the reward structure for the problem is\n",
    "            # We don't need the explicit overall reward as it's just the negative of the length of the trajectory\n",
    "\n",
    "            if state_action not in state_action_trajectory[:timestep]:\n",
    "                counts[*state_action] += 1\n",
    "                q_values[*state_action] += (current_return-q_values[*state_action])/counts[*state_action]\n",
    "                best_action = np.argmax(q_values[*state_action[:-1]])\n",
    "                policy[*state_action[:-1]] = default_probability\n",
    "                policy[*state_action[:-1], best_action] += (1-epsilon)\n",
    " \n",
    "        episode_length_history.append(len(state_action_trajectory))\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(episode_length_history, label=\"Episode Length\", color=\"red\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Length\")\n",
    "        plt.title(\"Monte Carlo On-policy progress\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "                \n",
    "\n",
    "on_policy_monte_carlo_control(race_track, behaviour_policy, behaviour_q_values, behaviour_policy_counts, gamma, epsilon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "cb506291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0.11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.005555555555555556"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((8, 3, 2) == (np.int64(8), np.int64(3), np.int64(1)))\n",
    "len(actions)\n",
    "print(default_probability)\n",
    "0.05/9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
