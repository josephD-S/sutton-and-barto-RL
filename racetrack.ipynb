{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16389d55",
   "metadata": {},
   "source": [
    "First it'd be best to setup the environment. I've copied the raw courses from: \n",
    "https://gist.github.com/pat-coady/26fafa10b4d14234bfde0bb58277786d\n",
    "\n",
    "Because this would take a while and be pretty tedious to do myself.\n",
    "\n",
    "But we'd still need to convert this to some other format. \n",
    "\n",
    "I think the best thing to do first would be to setup the environment, then from there work on the reinforcement learning part of it.\n",
    "\n",
    "Now that the environment is setup, we need to setup the target and behaviour policies.\n",
    "\n",
    "The target will just be a greedy deterministic policy.\n",
    "\n",
    "The behaviour policy will be epsilon greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e09a7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Racetracks\n",
    "\n",
    "# Left Race Track from Figure 5.5\n",
    "big_course = ['WWWWWWWWWWWWWWWWWW',\n",
    "              'WWWWooooooooooooo+',\n",
    "              'WWWoooooooooooooo+',\n",
    "              'WWWoooooooooooooo+',\n",
    "              'WWooooooooooooooo+',\n",
    "              'Woooooooooooooooo+',\n",
    "              'Woooooooooooooooo+',\n",
    "              'WooooooooooWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWWooooooWWWWWWWW',\n",
    "              'WWWWooooooWWWWWWWW',\n",
    "              'WWWW------WWWWWWWW']\n",
    "\n",
    "# Tiny course for debug\n",
    "\n",
    "tiny_course = ['WWWWWW',\n",
    "               'Woooo+',\n",
    "               'Woooo+',\n",
    "               'WooWWW',\n",
    "               'WooWWW',\n",
    "               'WooWWW',\n",
    "               'WooWWW',\n",
    "               'W--WWW',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bcac298",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a06fd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RaceTrack:\n",
    "    def __init__(self, course: list[str]):\n",
    "        self.course = self._load_course(course)\n",
    "        self.car_location = self._start()\n",
    "        self.velocity = (0, 0)\n",
    "        self.actions = [(y, x) for y in range(-1, 2) for x in range(-1, 2)]\n",
    "        self.action_idxs = np.arange(9)\n",
    "\n",
    "    def _pick_action(self, state: tuple[int, int, int, int], policy: np.array) -> int:\n",
    "        if policy[*state].shape == ():\n",
    "            return policy[*state]\n",
    "        else:\n",
    "            p_distribution = policy[*state]\n",
    "            # Action array should be of size 9\n",
    "            # Need to sample from it\n",
    "            action_idx = np.random.choice(self.action_idxs, 1, p=p_distribution)[0]\n",
    "        return action_idx\n",
    "\n",
    "    def _load_course(self, string_course: list[str]) -> np.array:\n",
    "        course = np.zeros((len(string_course), len(string_course[0])), dtype=np.int16)\n",
    "        course_dict = {\"W\": -1, \"o\": 0, \"-\": 1, \"+\": 2}\n",
    "\n",
    "        for i in range(len(course)):\n",
    "            for j in range(len(course[i])):\n",
    "                course[i, j] = course_dict[string_course[i][j]]\n",
    "\n",
    "        return course\n",
    "    \n",
    "    def _start(self) -> tuple[int, int]:\n",
    "        rows, cols = np.where(self.course == 1)\n",
    "        coords = list(zip(rows, cols))\n",
    "        start = random.randint(0, len(coords)-1)\n",
    "\n",
    "        return coords[start]\n",
    "    \n",
    "    def _restart(self): # If we hit the car boundary\n",
    "        self.car_location = self._start()\n",
    "        self.velocity = (0, 0)\n",
    "    \n",
    "    def _apply_velocity(self, action: tuple[int, int]):\n",
    "        y_vel = min(max(self.velocity[0] + action[0], 0), 5) # Bound to between 0-5\n",
    "        x_vel = min(max(self.velocity[1] + action[1], 0), 5)\n",
    "        self.velocity = (y_vel, x_vel)\n",
    "\n",
    "    def _apply_movement(self) -> str:\n",
    "        result = self._check_bounds()\n",
    "\n",
    "        if result == \"invalid\":\n",
    "            self._restart()\n",
    "            return \"restarted\"\n",
    "        elif result == \"complete\":\n",
    "            return \"done\"\n",
    "        else:\n",
    "            return \"continuing\"\n",
    "    \n",
    "    def _check_bounds(self) -> str:\n",
    "        y_vel, x_vel = self.velocity\n",
    "        y, x = self.car_location\n",
    "\n",
    "        max_x = len(self.course[0])\n",
    "        all_positions = [(y, x)]\n",
    "        max_vel = max(x_vel, y_vel)\n",
    "        y_vel_count, x_vel_count = y_vel, x_vel\n",
    "        y_temp, x_temp = y, x\n",
    "\n",
    "        for _ in range(max_vel):\n",
    "            if y_vel_count > 0 and x_vel_count > 0:\n",
    "                y_temp -= 1\n",
    "                x_temp += 1\n",
    "                if y_temp < 0 or x_temp >= max_x:\n",
    "                    return \"invalid\"\n",
    "                all_positions.append((y_temp, x_temp))\n",
    "                y_vel_count -= 1\n",
    "                x_vel_count -= 1\n",
    "            elif y_vel_count > 0:\n",
    "                y_temp -= 1\n",
    "                if y_temp < 0:\n",
    "                    return \"invalid\"\n",
    "                all_positions.append((y_temp, x_temp))\n",
    "                y_vel_count -= 1\n",
    "            else:\n",
    "                x_temp += 1\n",
    "                if x_temp >= max_x:\n",
    "                    return \"invalid\"\n",
    "                all_positions.append((y_temp, x_temp))\n",
    "                x_vel_count -= 1\n",
    "\n",
    "        all_position_values = []\n",
    "\n",
    "        for i, j in all_positions:\n",
    "            all_position_values.append(self.get_course()[i, j])\n",
    "\n",
    "        if -1 in all_position_values:\n",
    "            return \"invalid\"\n",
    "        elif 2 in all_position_values:\n",
    "            return \"complete\"\n",
    "        else:\n",
    "            self.car_location = all_positions[-1]\n",
    "            return \"valid\"\n",
    "\n",
    "    def get_course(self) -> np.array:\n",
    "        return self.course\n",
    "\n",
    "    def generate_random_episode(self): # For testing (outdated)\n",
    "        self.car_location = self._start()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action_idx = random.randint(0, len(self.actions)-1)\n",
    "            action = self.actions[action_idx]\n",
    "            self._apply_velocity(action)\n",
    "            result = self._apply_movement()\n",
    "            self.state_action_trajectory.append((self.car_location, action_idx))\n",
    "            self.running_reward -= 1\n",
    "\n",
    "            if result == \"done\":\n",
    "                done = True\n",
    "\n",
    "    def generate_episode(self, policy):\n",
    "        self._restart()\n",
    "        done = False\n",
    "        trajectory = []\n",
    "\n",
    "        while not done:\n",
    "            y, x = self.car_location\n",
    "            dy, dx = self.velocity\n",
    "            state = (y, x, dy, dx)\n",
    "\n",
    "            action_idx = self._pick_action(state, policy) # Take action\n",
    "            trajectory.append((*state, action_idx))\n",
    "\n",
    "            action = self.actions[action_idx]\n",
    "            self._apply_velocity(action)\n",
    "\n",
    "            result = self._apply_movement()\n",
    "            if result == \"done\":\n",
    "                done = True\n",
    "\n",
    "        return trajectory\n",
    "\n",
    "def mark_possible(possible_actions: np.array) -> np.array:\n",
    "    \"Marks impossible actions as 0\"\n",
    "    actions = np.array([(dy, dx) for dy in range(-1, 2) for dx in range(-1, 2)])\n",
    "    y_len, x_len, dy_len, dx_len, a_len = len(possible_actions), len(possible_actions[0]), 6, 6, len(actions)\n",
    "    for y in range(y_len):\n",
    "        for x in range(x_len):\n",
    "            for dy in range(dy_len):\n",
    "                for dx in range(dx_len):\n",
    "                    for a in range(a_len):\n",
    "                        dy_next, dx_next = actions[a]\n",
    "\n",
    "                        dy = min(max(dy + dy_next, 0), 5) # Bound to between 0-5\n",
    "                        dx = min(max(dx + dx_next, 0), 5)\n",
    "\n",
    "                        if dy == dx == 0:\n",
    "                            possible_actions[y, x, dy, dx, a] = 0\n",
    "\n",
    "    return possible_actions\n",
    "\n",
    "def init_equipropable_policy(policy: np.array, possible_actions: np.array) -> np.array:\n",
    "    \"\"\"Defines the equipropable policy where all possible actions have equal value\n",
    "    Keyword being possible, as many actions aren't possible due to it causing velocity to be (0, 0)\"\"\"\n",
    "    y_len, x_len, dy_len, dx_len = len(possible_actions), len(possible_actions[0]), 6, 6\n",
    "    for y in range(y_len):\n",
    "        for x in range(x_len):\n",
    "            for dy in range(dy_len):\n",
    "                for dx in range(dx_len):\n",
    "                    state = (y, x, dy, dx)\n",
    "                    possible_actions_sum = np.sum(possible_actions[*state])\n",
    "                    action_probabilty = 1/possible_actions_sum\n",
    "\n",
    "                    possible_actions_mask = possible_actions[*state] == 1\n",
    "                    policy[*state][possible_actions_mask] = action_probabilty \n",
    "\n",
    "\n",
    "    return policy\n",
    "\n",
    "def make_determinstic(policy: np.array) -> np.array:\n",
    "    policy = policy.copy()\n",
    "    y_len, x_len, dy_len, dx_len = len(policy), len(policy[0]), 6, 6\n",
    "    for y in range(y_len):\n",
    "        for x in range(x_len):\n",
    "            for dy in range(dy_len):\n",
    "                for dx in range(dx_len):\n",
    "                    state = (y, x, dy, dx)\n",
    "                    best_a_idx = np.argmax(policy[*state])\n",
    "                    best_a = policy[*state, best_a_idx]\n",
    "                    mask = policy[*state] == best_a \n",
    "                    policy[*state] = np.where(mask, 1, 0)\n",
    "    \n",
    "    return policy\n",
    "\n",
    "\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9905c3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 112\u001b[39m\n\u001b[32m    108\u001b[39m         plt.tight_layout()\n\u001b[32m    109\u001b[39m         plt.show()\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m behaviour_policy = \u001b[43mon_policy_monte_carlo_control\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrace_track\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehaviour_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehaviour_q_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehaviour_policy_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGAMMA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPSILON\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTHETA\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mon_policy_monte_carlo_control\u001b[39m\u001b[34m(race_track, policy, q_values, counts, gamma, epsilon, possible_actions, theta, max_iterations)\u001b[39m\n\u001b[32m    106\u001b[39m ax2.set_xlim(start, \u001b[38;5;28mmax\u001b[39m(start + \u001b[32m1\u001b[39m, n - \u001b[32m1\u001b[39m))\n\u001b[32m    108\u001b[39m plt.tight_layout()\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/matplotlib/pyplot.py:614\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    570\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[33;03mDisplay all open figures.\u001b[39;00m\n\u001b[32m    572\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    611\u001b[39m \u001b[33;03mexplicitly there.\u001b[39;00m\n\u001b[32m    612\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    613\u001b[39m _warn_if_gui_out_of_main_thread()\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/matplotlib_inline/backend_inline.py:90\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(close, block)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf.get_all_fig_managers():\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcanvas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcanvas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     95\u001b[39m     show._to_draw = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/IPython/core/display_functions.py:278\u001b[39m, in \u001b[36mdisplay\u001b[39m\u001b[34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[39m\n\u001b[32m    276\u001b[39m     publish_display_data(data=obj, metadata=metadata, **kwargs)\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     format_dict, md_dict = \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[32m    280\u001b[39m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[32m    281\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/IPython/core/formatters.py:238\u001b[39m, in \u001b[36mDisplayFormatter.format\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m    236\u001b[39m md = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     data = \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/decorator.py:235\u001b[39m, in \u001b[36mdecorate.<locals>.fun\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[32m    234\u001b[39m     args, kw = fix(args, kw, sig)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/IPython/core/formatters.py:282\u001b[39m, in \u001b[36mcatch_format_error\u001b[39m\u001b[34m(method, self, *args, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     r = \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[32m    284\u001b[39m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/IPython/core/formatters.py:402\u001b[39m, in \u001b[36mBaseFormatter.__call__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[32m    404\u001b[39m method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/IPython/core/pylabtools.py:170\u001b[39m, in \u001b[36mprint_figure\u001b[39m\u001b[34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[39m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[32m    168\u001b[39m     FigureCanvasBase(fig)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcanvas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m data = bytes_io.getvalue()\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fmt == \u001b[33m'\u001b[39m\u001b[33msvg\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/matplotlib/backend_bases.py:2160\u001b[39m, in \u001b[36mFigureCanvasBase.print_figure\u001b[39m\u001b[34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[39m\n\u001b[32m   2158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[32m   2159\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches == \u001b[33m\"\u001b[39m\u001b[33mtight\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2160\u001b[39m         bbox_inches = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2161\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2162\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(layout_engine, ConstrainedLayoutEngine) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m   2163\u001b[39m                 pad_inches == \u001b[33m\"\u001b[39m\u001b[33mlayout\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2164\u001b[39m             h_pad = layout_engine.get()[\u001b[33m\"\u001b[39m\u001b[33mh_pad\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/matplotlib/figure.py:1848\u001b[39m, in \u001b[36mFigureBase.get_tightbbox\u001b[39m\u001b[34m(self, renderer, bbox_extra_artists)\u001b[39m\n\u001b[32m   1844\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ax.get_visible():\n\u001b[32m   1845\u001b[39m     \u001b[38;5;66;03m# some Axes don't take the bbox_extra_artists kwarg so we\u001b[39;00m\n\u001b[32m   1846\u001b[39m     \u001b[38;5;66;03m# need this conditional....\u001b[39;00m\n\u001b[32m   1847\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1848\u001b[39m         bbox = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1849\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1850\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1851\u001b[39m         bbox = ax.get_tightbbox(renderer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/matplotlib/axes/_base.py:4564\u001b[39m, in \u001b[36m_AxesBase.get_tightbbox\u001b[39m\u001b[34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[39m\n\u001b[32m   4562\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._axis_map.values():\n\u001b[32m   4563\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.axison \u001b[38;5;129;01mand\u001b[39;00m axis.get_visible():\n\u001b[32m-> \u001b[39m\u001b[32m4564\u001b[39m         ba = \u001b[43mmartist\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_tightbbox_for_layout_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4565\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m ba:\n\u001b[32m   4566\u001b[39m             bb.append(ba)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/matplotlib/artist.py:1402\u001b[39m, in \u001b[36m_get_tightbbox_for_layout_only\u001b[39m\u001b[34m(obj, *args, **kwargs)\u001b[39m\n\u001b[32m   1396\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1397\u001b[39m \u001b[33;03mMatplotlib's `.Axes.get_tightbbox` and `.Axis.get_tightbbox` support a\u001b[39;00m\n\u001b[32m   1398\u001b[39m \u001b[33;03m*for_layout_only* kwarg; this helper tries to use the kwarg but skips it\u001b[39;00m\n\u001b[32m   1399\u001b[39m \u001b[33;03mwhen encountering third-party subclasses that do not support it.\u001b[39;00m\n\u001b[32m   1400\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfor_layout_only\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1404\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.get_tightbbox(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/matplotlib/axis.py:1351\u001b[39m, in \u001b[36mAxis.get_tightbbox\u001b[39m\u001b[34m(self, renderer, for_layout_only)\u001b[39m\n\u001b[32m   1349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1350\u001b[39m     renderer = \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m)._get_renderer()\n\u001b[32m-> \u001b[39m\u001b[32m1351\u001b[39m ticks_to_draw = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_ticks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[38;5;28mself\u001b[39m._update_label_position(renderer)\n\u001b[32m   1355\u001b[39m \u001b[38;5;66;03m# go back to just this axis's tick labels\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/matplotlib/axis.py:1282\u001b[39m, in \u001b[36mAxis._update_ticks\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1277\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1278\u001b[39m \u001b[33;03mUpdate ticks (position and labels) using the current data interval of\u001b[39;00m\n\u001b[32m   1279\u001b[39m \u001b[33;03mthe axes.  Return the list of ticks that will be drawn.\u001b[39;00m\n\u001b[32m   1280\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1281\u001b[39m major_locs = \u001b[38;5;28mself\u001b[39m.get_majorticklocs()\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m major_labels = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmajor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_ticks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmajor_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1283\u001b[39m major_ticks = \u001b[38;5;28mself\u001b[39m.get_major_ticks(\u001b[38;5;28mlen\u001b[39m(major_locs))\n\u001b[32m   1284\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tick, loc, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(major_ticks, major_locs, major_labels):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/matplotlib/ticker.py:216\u001b[39m, in \u001b[36mFormatter.format_ticks\u001b[39m\u001b[34m(self, values)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_ticks\u001b[39m(\u001b[38;5;28mself\u001b[39m, values):\n\u001b[32m    215\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the tick labels for all the ticks at once.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_locs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m(value, i) \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/matplotlib/ticker.py:742\u001b[39m, in \u001b[36mScalarFormatter.set_locs\u001b[39m\u001b[34m(self, locs)\u001b[39m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.locs) > \u001b[32m0\u001b[39m:\n\u001b[32m    741\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._useOffset:\n\u001b[32m--> \u001b[39m\u001b[32m742\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_offset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    743\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_order_of_magnitude()\n\u001b[32m    744\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_format()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/matplotlib/ticker.py:749\u001b[39m, in \u001b[36mScalarFormatter._compute_offset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    747\u001b[39m locs = \u001b[38;5;28mself\u001b[39m.locs\n\u001b[32m    748\u001b[39m \u001b[38;5;66;03m# Restrict to visible ticks.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m vmin, vmax = \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_view_interval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    750\u001b[39m locs = np.asarray(locs)\n\u001b[32m    751\u001b[39m locs = locs[(vmin <= locs) & (locs <= vmax)]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "course = big_course\n",
    "actions = np.array([(dy, dx) for dy in range(-1, 2) for dx in range(-1, 2)])\n",
    "\n",
    "# Define constants\n",
    "MAX_VELOCITY = 5\n",
    "GAMMA = 1 # Discount rate\n",
    "EPSILON = 0.05 # Exploration factor for on-policy\n",
    "THETA = 1e-3 # Breaking factor\n",
    "\n",
    "# Setup behaviour policy\n",
    "# Actions here could be represented as an array of size (9) or (3, 3)\n",
    "# If it were 9, then the actions are grouped to be (-1, -1), (-1, 0), ..., (1, 1) etc.\n",
    "# If it were (3, 3), then there's 3 possible actions for each axis, but I think this would be more annoying to implement\n",
    "\n",
    "behaviour_policy = np.zeros((len(course), len(course[0]), MAX_VELOCITY+1, MAX_VELOCITY+1, len(actions))) # Begin as equiprobable policy, will be stochastic\n",
    "behaviour_q_values = np.zeros(behaviour_policy.shape)\n",
    "behaviour_policy_counts = np.zeros(behaviour_policy.shape)\n",
    "\n",
    "possible_actions = np.zeros((len(course), len(course[0]), MAX_VELOCITY+1, MAX_VELOCITY+1, len(actions))) + 1 # Initially all actions are possible\n",
    "possible_actions = mark_possible(possible_actions) # Mark impossible actions (impossilbe as per the problem description)\n",
    "\n",
    "behaviour_policy = init_equipropable_policy(behaviour_policy, possible_actions) # Initialise all possilbe actions with equal possibility\n",
    "\n",
    "race_track = RaceTrack(course)\n",
    "\n",
    "plt.ion()\n",
    "def on_policy_monte_carlo_control(race_track: RaceTrack, policy: np.array, q_values: np.array, counts: np.array, \n",
    "                                  gamma: float, epsilon: float, possible_actions: np.array, theta, \n",
    "                                  max_iterations: int = 200_000) -> np.array:\n",
    "    episode_length_history = []\n",
    "    episode_average_history = []\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        state_action_trajectory = race_track.generate_episode(policy)\n",
    "        current_return = 0\n",
    "        timestep = len(state_action_trajectory)\n",
    "        old_policy = policy.copy()\n",
    "        old_q = q_values.copy()\n",
    "\n",
    "        for state_action in reversed(state_action_trajectory):\n",
    "            current_return = gamma * current_return + -1 # Taking advantage of how the reward structure for the problem is\n",
    "            # We don't need the explicit overall reward as it's just the negative of the length of the trajectory\n",
    "            timestep -= 1\n",
    "            \n",
    "            \"\"\"\n",
    "            If we want to use first-visit, we'd add this line:\n",
    "            if state_action not in state_action_trajectory[:timestep]:\n",
    "            \n",
    "            However this makes it MUCH slower, increasing it from O(n) to O(n^2) as we need to check the whole trajectory for each timestep.\n",
    "            So every-visit is MUCH faster in practice as it's O(n)\n",
    "\n",
    "            \"\"\"\n",
    "            counts[*state_action] += 1\n",
    "            q_values[*state_action] += (current_return-q_values[*state_action])/counts[*state_action]\n",
    "            \n",
    "            # Get Argmax (vectorised)\n",
    "            possible_mask = possible_actions[*state_action[:-1]] == 1 \n",
    "            best_a_idx = np.argmax(np.where(possible_mask, q_values[*state_action[:-1]], -np.inf))\n",
    "            \n",
    "            possible_actions_sum = np.sum(possible_actions[*state_action[:-1]])\n",
    "            random_probability = epsilon/possible_actions_sum\n",
    "\n",
    "            # Assign probabilities (vectorised)\n",
    "            policy[*state_action[:-1]][possible_mask] = random_probability\n",
    "            policy[*state_action[:-1], best_a_idx] += (1-epsilon)\n",
    "\n",
    "        if np.all(old_policy == policy) and np.allclose(q_values, old_q, atol=theta): # Breaking condition\n",
    "            return policy\n",
    "\n",
    "        episode_length_history.append(len(state_action_trajectory))\n",
    "        episode_average_history.append(sum(episode_length_history)/len(episode_length_history))\n",
    "\n",
    "        window = 500\n",
    "\n",
    "        n = len(episode_average_history)\n",
    "        start = max(0, n-window)\n",
    "\n",
    "        x_full = np.arange(n)\n",
    "        x_win = x_full[start:]\n",
    "\n",
    "        y_win = np.array(episode_average_history[start:])\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
    "\n",
    "        # --- Subplot 1: Episode Lengths ---\n",
    "        ax1.plot(episode_length_history, color=\"red\", label=\"Episode Length\")\n",
    "        ax1.set_xlabel(\"Episode\")\n",
    "        ax1.set_ylabel(\"Length\")\n",
    "        ax1.set_title(\"Episode Length per Episode\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # --- Subplot 2: Average Lengths ---\n",
    "        ax2.plot(x_win, y_win, color=\"blue\", label=\"Average Length\")\n",
    "        ax2.set_xlabel(\"Episode\")\n",
    "        ax2.set_ylabel(\"Average Length\")\n",
    "        ax2.set_title(\"Running Average Episode Length\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "\n",
    "        ax2.set_xlim(start, max(start + 1, n - 1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "behaviour_policy = on_policy_monte_carlo_control(race_track, behaviour_policy, behaviour_q_values, behaviour_policy_counts, GAMMA, EPSILON, possible_actions, THETA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48dae244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(race_track.generate_episode(behaviour_policy)))\n\u001b[32m      3\u001b[39m deterministic_on_policy = make_determinstic(behaviour_policy)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mrace_track\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeterministic_on_policy\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 129\u001b[39m, in \u001b[36mRaceTrack.generate_episode\u001b[39m\u001b[34m(self, policy)\u001b[39m\n\u001b[32m    126\u001b[39m dy, dx = \u001b[38;5;28mself\u001b[39m.velocity\n\u001b[32m    127\u001b[39m state = (y, x, dy, dx)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m action_idx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pick_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Take action\u001b[39;00m\n\u001b[32m    130\u001b[39m trajectory.append((*state, action_idx))\n\u001b[32m    132\u001b[39m action = \u001b[38;5;28mself\u001b[39m.actions[action_idx]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mRaceTrack._pick_action\u001b[39m\u001b[34m(self, state, policy)\u001b[39m\n\u001b[32m     13\u001b[39m     p_distribution = policy[*state]\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Action array should be of size 9\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Need to sample from it\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     action_idx = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_idxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_distribution\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m action_idx\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:1001\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.choice\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/RL/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3323\u001b[39m, in \u001b[36m_prod_dispatcher\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   3309\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3310\u001b[39m \u001b[33;03m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[32m   3311\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3317\u001b[39m \u001b[33;03m    ndarray.min : equivalent method\u001b[39;00m\n\u001b[32m   3318\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3319\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np.minimum, \u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out,\n\u001b[32m   3320\u001b[39m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[32m-> \u001b[39m\u001b[32m3323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prod_dispatcher\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   3324\u001b[39m                      initial=\u001b[38;5;28;01mNone\u001b[39;00m, where=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   3325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[32m   3328\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_prod_dispatcher)\n\u001b[32m   3329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprod\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue,\n\u001b[32m   3330\u001b[39m          initial=np._NoValue, where=np._NoValue):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(len(race_track.generate_episode(behaviour_policy)))\n",
    "\n",
    "deterministic_on_policy = make_determinstic(behaviour_policy)\n",
    "\n",
    "print(len(race_track.generate_episode(deterministic_on_policy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "159940f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Off policy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m target_q_values = \u001b[43mnp\u001b[49m.zeros((\u001b[38;5;28mlen\u001b[39m(course), \u001b[38;5;28mlen\u001b[39m(course[\u001b[32m0\u001b[39m]), MAX_VELOCITY+\u001b[32m1\u001b[39m, MAX_VELOCITY+\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(actions))) \u001b[38;5;66;03m# len(x), len(y), 6 speeds up and down (0-6), 9 possible actions. If it were state values, we'd just need v = x, y, 5, 5\u001b[39;00m\n\u001b[32m      6\u001b[39m target_policy = np.zeros((\u001b[38;5;28mlen\u001b[39m(course), \u001b[38;5;28mlen\u001b[39m(course[\u001b[32m0\u001b[39m]), MAX_VELOCITY+\u001b[32m1\u001b[39m, MAX_VELOCITY+\u001b[32m1\u001b[39m), dtype=np.int32) + \u001b[32m4\u001b[39m \u001b[38;5;66;03m# Need an array for each state, default value set to (0, 0), no need for 9 actions per state as it's deterministic, so just set it as the index\u001b[39;00m\n\u001b[32m      7\u001b[39m c_values = np.zeros(target_q_values.shape) \u001b[38;5;66;03m# Need the same shape for our running importance sampling sum\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Off policy\n",
    "\n",
    "target_q_values = np.zeros((len(course), len(course[0]), MAX_VELOCITY+1, MAX_VELOCITY+1, len(actions))) # len(x), len(y), 6 speeds up and down (0-6), 9 possible actions. If it were state values, we'd just need v = x, y, 5, 5\n",
    "target_policy = np.zeros((len(course), len(course[0]), MAX_VELOCITY+1, MAX_VELOCITY+1), dtype=np.int32) + 4 # Need an array for each state, default value set to (0, 0), no need for 9 actions per state as it's deterministic, so just set it as the index\n",
    "c_values = np.zeros(target_q_values.shape) # Need the same shape for our running importance sampling sum\n",
    "\n",
    "race_track = RaceTrack(course)\n",
    "\n",
    "def off_policy_monte_carlo(race_track: RaceTrack, target_policy: np.array, target_q_values: np.array, behaviour_policy: np.array,\n",
    "                            possible_actions: np.array, gamma: float, max_iterations: int = 50_000) -> np.array:\n",
    "    c_values = np.zeros(target_q_values.shape)\n",
    "\n",
    "    for i in tqdm(range(max_iterations)):\n",
    "        traj = race_track.generate_episode(behaviour_policy)\n",
    "        G = 0 # Return\n",
    "        W = 1 # Running importance sampling ratio\n",
    "\n",
    "        for state_action in reversed(traj):\n",
    "            G = gamma * G + -1\n",
    "            c_values[*state_action] += W \n",
    "            target_q_values[*state_action] += (W/c_values[*state_action]) * (G-target_q_values[*state_action])\n",
    "            \n",
    "            possible_actions_mask = possible_actions[*state_action[:-1]] == 1\n",
    "            best_a_idx = np.argmax(np.where(possible_actions_mask, target_q_values[*state_action[:-1]], -np.inf))\n",
    "            target_policy[*state_action[:-1]] = best_a_idx\n",
    "\n",
    "            if target_policy[*state_action[:-1]] != state_action[-1]:\n",
    "                break \n",
    "            else:\n",
    "                W =  W * (1/behaviour_policy[state_action])\n",
    "\n",
    "    return target_policy\n",
    "\n",
    "\n",
    "target_policy = off_policy_monte_carlo(race_track, target_policy, target_q_values, behaviour_policy, possible_actions, GAMMA)\n",
    "print(\"Reward: \", len(race_track.generate_episode(target_policy)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "006c98c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9,)\n",
      "()\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(behaviour_policy[0, 0, 0, 0].shape)\n",
    "print(target_policy[0, 0, 0, 0].shape)\n",
    "\n",
    "print(target_policy[0, 0, 0, 0].shape == ())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
