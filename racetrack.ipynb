{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16389d55",
   "metadata": {},
   "source": [
    "First it'd be best to setup the environment. I've copied the raw courses from: \n",
    "https://gist.github.com/pat-coady/26fafa10b4d14234bfde0bb58277786d\n",
    "\n",
    "Because this would take a while and be pretty tedious to do myself.\n",
    "\n",
    "But we'd still need to convert this to some other format. \n",
    "\n",
    "I think the best thing to do first would be to setup the environment, then from there work on the reinforcement learning part of it.\n",
    "\n",
    "Now that the environment is setup, we need to setup the target and behaviour policies.\n",
    "\n",
    "The target will just be a greedy deterministic policy.\n",
    "\n",
    "The behaviour policy will be epsilon greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09a7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Racetracks\n",
    "\n",
    "# Left Race Track from Figure 5.5\n",
    "big_course = ['WWWWWWWWWWWWWWWWWW',\n",
    "              'WWWWooooooooooooo+',\n",
    "              'WWWoooooooooooooo+',\n",
    "              'WWWoooooooooooooo+',\n",
    "              'WWooooooooooooooo+',\n",
    "              'Woooooooooooooooo+',\n",
    "              'Woooooooooooooooo+',\n",
    "              'WooooooooooWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WoooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWooooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWoooooooWWWWWWWW',\n",
    "              'WWWWooooooWWWWWWWW',\n",
    "              'WWWWooooooWWWWWWWW',\n",
    "              'WWWW------WWWWWWWW']\n",
    "\n",
    "# Tiny course for debug\n",
    "\n",
    "tiny_course = ['WWWWWW',\n",
    "               'Woooo+',\n",
    "               'Woooo+',\n",
    "               'WooWWW',\n",
    "               'WooWWW',\n",
    "               'WooWWW',\n",
    "               'WooWWW',\n",
    "               'W--WWW',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bcac298",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a06fd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RaceTrack:\n",
    "    def __init__(self, course: list[str], possible_actions: np.array):\n",
    "        self.course = self._load_course(course)\n",
    "        self.car_location = self._start()\n",
    "        self.velocity = (0, 0)\n",
    "        self.actions = [(y, x) for y in range(-1, 2) for x in range(-1, 2)]\n",
    "        self.action_idxs = np.arange(9)\n",
    "        self.possible_actions = possible_actions\n",
    "\n",
    "    def _pick_action(self, state: tuple[int, int, int, int], policy: np.array) -> int:\n",
    "        p_distribution = policy[*state]\n",
    "        # Action array should be of size 9\n",
    "        # Need to sample from it\n",
    "        action_idx = np.random.choice(self.action_idxs, 1, p=p_distribution)[0]\n",
    "        return action_idx\n",
    "\n",
    "    def _load_course(self, string_course: list[str]) -> np.array:\n",
    "        course = np.zeros((len(string_course), len(string_course[0])), dtype=np.int16)\n",
    "        course_dict = {\"W\": -1, \"o\": 0, \"-\": 1, \"+\": 2}\n",
    "\n",
    "        for i in range(len(course)):\n",
    "            for j in range(len(course[i])):\n",
    "                course[i, j] = course_dict[string_course[i][j]]\n",
    "\n",
    "        return course\n",
    "    \n",
    "    def _start(self) -> tuple[int, int]:\n",
    "        rows, cols = np.where(self.course == 1)\n",
    "        coords = list(zip(rows, cols))\n",
    "        start = random.randint(0, len(coords)-1)\n",
    "\n",
    "        return coords[start]\n",
    "    \n",
    "    def _restart(self): # If we hit the car boundary\n",
    "        self.car_location = self._start()\n",
    "        self.velocity = (0, 0)\n",
    "    \n",
    "    def _apply_velocity(self, action: tuple[int, int]):\n",
    "        y_vel = min(max(self.velocity[0] + action[0], 0), 5) # Bound to between 0-5\n",
    "        x_vel = min(max(self.velocity[1] + action[1], 0), 5)\n",
    "        self.velocity = (y_vel, x_vel)\n",
    "\n",
    "    def _apply_movement(self) -> str:\n",
    "        result = self._check_bounds()\n",
    "\n",
    "        if result == \"invalid\":\n",
    "            self._restart()\n",
    "            return \"restarted\"\n",
    "        elif result == \"complete\":\n",
    "            return \"done\"\n",
    "        else:\n",
    "            return \"continuing\"\n",
    "    \n",
    "    def _check_bounds(self) -> str:\n",
    "        y_vel, x_vel = self.velocity\n",
    "        y, x = self.car_location\n",
    "\n",
    "        max_x = len(self.course[0])\n",
    "        all_positions = [(y, x)]\n",
    "        max_vel = max(x_vel, y_vel)\n",
    "        y_vel_count, x_vel_count = y_vel, x_vel\n",
    "        y_temp, x_temp = y, x\n",
    "\n",
    "        for _ in range(max_vel):\n",
    "            if y_vel_count > 0 and x_vel_count > 0:\n",
    "                y_temp -= 1\n",
    "                x_temp += 1\n",
    "                if y_temp < 0 or x_temp >= max_x:\n",
    "                    return \"invalid\"\n",
    "                all_positions.append((y_temp, x_temp))\n",
    "                y_vel_count -= 1\n",
    "                x_vel_count -= 1\n",
    "            elif y_vel_count > 0:\n",
    "                y_temp -= 1\n",
    "                if y_temp < 0:\n",
    "                    return \"invalid\"\n",
    "                all_positions.append((y_temp, x_temp))\n",
    "                y_vel_count -= 1\n",
    "            else:\n",
    "                x_temp += 1\n",
    "                if x_temp >= max_x:\n",
    "                    return \"invalid\"\n",
    "                all_positions.append((y_temp, x_temp))\n",
    "                x_vel_count -= 1\n",
    "\n",
    "        all_position_values = []\n",
    "\n",
    "        for i, j in all_positions:\n",
    "            all_position_values.append(self.get_course()[i, j])\n",
    "\n",
    "        if -1 in all_position_values:\n",
    "            return \"invalid\"\n",
    "        elif 2 in all_position_values:\n",
    "            return \"complete\"\n",
    "        else:\n",
    "            self.car_location = all_positions[-1]\n",
    "            return \"valid\"\n",
    "\n",
    "    def get_course(self) -> np.array:\n",
    "        return self.course\n",
    "\n",
    "    def generate_random_episode(self): # For testing\n",
    "        self.car_location = self._start()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action_idx = random.randint(0, len(self.actions)-1)\n",
    "            action = self.actions[action_idx]\n",
    "            self._apply_velocity(action)\n",
    "            result = self._apply_movement()\n",
    "            self.state_action_trajectory.append((self.car_location, action_idx))\n",
    "            self.running_reward -= 1\n",
    "\n",
    "            if result == \"done\":\n",
    "                done = True\n",
    "\n",
    "    def generate_episode(self, policy):\n",
    "        self._restart()\n",
    "        done = False\n",
    "        state_action_trajectory = []\n",
    "\n",
    "        while not done:\n",
    "            y, x = self.car_location\n",
    "            y_vel, x_vel = self.velocity\n",
    "            state = (y, x, y_vel, x_vel)\n",
    "\n",
    "            action_idx = self._pick_action(state, policy) # Take action\n",
    "            state_action_trajectory.append((*self.car_location, *self.velocity, action_idx))\n",
    "\n",
    "            action = self.actions[action_idx]\n",
    "            self._apply_velocity(action)\n",
    "\n",
    "            if self.velocity == (0, 0):\n",
    "                print(\"Velocity is 0\")\n",
    "\n",
    "            result = self._apply_movement()\n",
    "            if result == \"done\":\n",
    "                done = True\n",
    "\n",
    "        return state_action_trajectory\n",
    "\n",
    "def mark_possible(possible_actions: np.array) -> np.array:\n",
    "    \"Marks impossible actions as 0\"\n",
    "    actions = np.array([(y, x) for y in range(-1, 2) for x in range(-1, 2)])\n",
    "    y_len, x_len, y_vel_len, x_vel_len, actions_len = len(possible_actions), len(possible_actions[0]), 6, 6, 9\n",
    "    for y in range(y_len):\n",
    "        for x in range(x_len):\n",
    "            for y_velocity in range(y_vel_len):\n",
    "                for x_velocity in range(x_vel_len):\n",
    "                    for a in range(actions_len):\n",
    "                        prior_velocity = np.array([y_velocity, x_velocity])\n",
    "                        action = actions[a]\n",
    "\n",
    "                        y_vel = min(max(prior_velocity[0] + action[0], 0), 5) # Bound to between 0-5\n",
    "                        x_vel = min(max(prior_velocity[1] + action[1], 0), 5)\n",
    "\n",
    "                        if y_vel == x_vel == 0:\n",
    "                            possible_actions[y, x, y_velocity, x_velocity, a] = 0\n",
    "                            #print(\"State-action, \", y, x, y_velocity, x_velocity, a, \" is not possilbe\")\n",
    "    return possible_actions\n",
    "\n",
    "def init_equipropable_policy(policy: np.array, possible_actions: np.array) -> np.array:\n",
    "    \"\"\"Defines the equipropable policy where all possible actions have equal value\n",
    "    Keyword being possible, as many actions aren't possible due to it causing velocity to be (0, 0)\"\"\"\n",
    "    y_len, x_len, y_vel_len, x_vel_len, action_len = len(possible_actions), len(possible_actions[0]), 6, 6, 9\n",
    "    for y in range(y_len):\n",
    "        for x in range(x_len):\n",
    "            for y_velocity in range(y_vel_len):\n",
    "                for x_velocity in range(x_vel_len):\n",
    "                    state = (y, x, y_velocity, x_velocity)\n",
    "                    possible_actions_sum = np.sum(possible_actions[*state])\n",
    "                    action_probabilty = 1/possible_actions_sum\n",
    "                    for a in range(action_len):\n",
    "                        if possible_actions[*state, a]:\n",
    "                            policy[*state, a] = action_probabilty\n",
    "                        else:\n",
    "                            policy[*state, a] = 0\n",
    "\n",
    "    return policy\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9905c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     67\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTEST\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m        episode_length_history.append(len(state_action_trajectory))\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[33;03m        clear_output(wait=True)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m \u001b[33;03m        plt.show()\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[43mon_policy_monte_carlo_control\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrace_track\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehaviour_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehaviour_q_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehaviour_policy_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_actions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mon_policy_monte_carlo_control\u001b[39m\u001b[34m(race_track, policy, q_values, counts, gamma, epsilon, possible_actions, iterations)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# We don't need the explicit overall reward as it's just the negative of the length of the trajectory\u001b[39;00m\n\u001b[32m     45\u001b[39m timestep -= \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state_action \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m state_action_trajectory[:timestep]:\n\u001b[32m     47\u001b[39m     counts[*state_action] += \u001b[32m1\u001b[39m\n\u001b[32m     48\u001b[39m     q_values[*state_action] += (current_return-q_values[*state_action])/counts[*state_action]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "course = big_course\n",
    "\n",
    "actions = np.array([(y, x) for y in range(-1, 2) for x in range(-1, 2)])\n",
    "\n",
    "# Setup target and behaviour policies\n",
    "# Actions here could be represented as an array of size (9) or (3, 3)\n",
    "# If it were 9, then the actions are grouped to be (-1, -1), (-1, 0), ..., (1, 1) etc.\n",
    "# If it were (3, 3), then there's 3 possible actions for each axis, but I think this would be more annoying to implement\n",
    "\n",
    "target_q_values = np.zeros((len(course), len(course[0]), 6, 6, 9)) # len(x), len(y), 6 speeds up and down (0-6), 9 possible actions. If it were state values, we'd just need v = x, y, 5, 5\n",
    "target_policy = np.zeros((len(course), len(course[0]), 6, 6, 9)) + 4 # Need an array for each state, default value set to (0, 0)\n",
    "c_values = np.zeros(target_q_values.shape) # Need the same shape for our running importance sampling sum\n",
    "\n",
    "behaviour_policy = np.zeros((len(course), len(course[0]), 6, 6, 9)) # Begin as equiprobable policy, will be stochastic\n",
    "behaviour_q_values = np.zeros(behaviour_policy.shape)\n",
    "behaviour_policy_counts = np.zeros(behaviour_policy.shape)\n",
    "\n",
    "possible_actions = np.zeros((len(course), len(course[0]), 6, 6, 9)) + 1 # Initially all actions are possible\n",
    "possible_actions = mark_possible(possible_actions)\n",
    "\n",
    "behaviour_policy = init_equipropable_policy(behaviour_policy, possible_actions)\n",
    "\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "default_probability = epsilon/len(actions)\n",
    "\n",
    "race_track = RaceTrack(course, possible_actions)\n",
    "\n",
    "plt.ion()\n",
    "def on_policy_monte_carlo_control(race_track: RaceTrack, policy: np.array, q_values: np.array, counts: np.array, \n",
    "                                  gamma: float, epsilon: float, possible_actions: np.array, iterations: int = 100000):\n",
    "    episode_length_history = []\n",
    "    for i in range(iterations):\n",
    "        state_action_trajectory = race_track.generate_episode(policy)\n",
    "        current_return = 0\n",
    "        timestep = len(state_action_trajectory)\n",
    "\n",
    "        for state_action in reversed(state_action_trajectory):\n",
    "            current_return = gamma * current_return + -1 # Taking advantage of how the reward structure for the problem is\n",
    "            # We don't need the explicit overall reward as it's just the negative of the length of the trajectory\n",
    "            timestep -= 1\n",
    "            if state_action not in state_action_trajectory[:timestep]:\n",
    "                counts[*state_action] += 1\n",
    "                q_values[*state_action] += (current_return-q_values[*state_action])/counts[*state_action]\n",
    "                \n",
    "                best_action_q_value = -np.inf\n",
    "                best_action = None\n",
    "                for a in range(9):\n",
    "                    if possible_actions[*state_action[:-1], a] == 1:\n",
    "                        if q_values[*state_action[:-1], a] > best_action_q_value:\n",
    "                            best_action_q_value = q_values[*state_action[:-1], a]\n",
    "                            best_action = a\n",
    "                \n",
    "                possible_actions_sum = np.sum(possible_actions[*state_action[:-1]])\n",
    "                random_probability = epsilon/possible_actions_sum\n",
    "                for a in range(9):\n",
    "                    if possible_actions[*state_action[:-1], a] == 1:\n",
    "                        policy[*state_action[:-1], a] = random_probability\n",
    "                    else:\n",
    "                        policy[*state_action[:-1], a] = 0\n",
    "                policy[*state_action[:-1], best_action] += (1-epsilon)\n",
    "\n",
    "\n",
    "        episode_length_history.append(len(state_action_trajectory))\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(episode_length_history, label=\"Episode Length\", color=\"red\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Length\")\n",
    "        plt.title(\"Monte Carlo On-policy progress\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "                \n",
    "on_policy_monte_carlo_control(race_track, behaviour_policy, behaviour_q_values, behaviour_policy_counts, gamma, epsilon, possible_actions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "e0f4c71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00000004\n"
     ]
    }
   ],
   "source": [
    "print(np.sum([0.95555556, 0.00555556, 0.00555556, 0.00555556, 0.00555556, 0.00555556, 0.00555556, 0.00555556, 0.00555556]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
